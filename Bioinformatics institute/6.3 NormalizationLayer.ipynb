{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-norm I\n",
    "\n",
    "In this step, you need to implement the batch normalization function without using the standard function with the following simplifications:\n",
    "\n",
    "    The Beta parameter is taken equal to 0.\n",
    "    The Gamma parameter is assumed to be 1.\n",
    "    The function should work correctly only at the training stage.\n",
    "    The input has the dimension of the number of elements in the batch * the length of each instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def custom_batch_norm1d(input_tensor, eps):\n",
    "    normed_tensor = input_tensor\n",
    "    m = torch.mean(input_tensor, dim=0)\n",
    "    s = torch.var(input_tensor, dim=0, unbiased=False)\n",
    "    normed_tensor = (input_tensor - m) / torch.sqrt(s + eps) # Напишите в этом месте нормирование входного тензора\n",
    "    return normed_tensor\n",
    "\n",
    "\n",
    "input_tensor = torch.Tensor([[0.0, 0, 1, 0, 2], [0, 1, 1, 0, 10]])\n",
    "batch_norm = nn.BatchNorm1d(input_tensor.shape[1], affine=False)\n",
    "\n",
    "\n",
    "all_correct = True\n",
    "for eps_power in range(10):\n",
    "     eps = np.power(10., -eps_power)\n",
    "     batch_norm.eps = eps\n",
    "     batch_norm_out = batch_norm(input_tensor)\n",
    "     custom_batch_norm_out = custom_batch_norm1d(input_tensor, eps)\n",
    "\n",
    "     all_correct &= torch.allclose(batch_norm_out, custom_batch_norm_out)\n",
    "     all_correct &= batch_norm_out.shape == custom_batch_norm_out.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-norm II\n",
    "\n",
    "Let's generalize the function from the previous step a bit - let's add the ability to set Beta and Gamma parameters.\n",
    "\n",
    "At this step, you need to implement the batch normalization function without using the standard function with the following simplifications:\n",
    "\n",
    "    The function should work correctly only at the training stage.\n",
    "    The input has the dimension of the number of elements in the batch * the length of each instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 7\n",
    "batch_size = 5\n",
    "input_tensor = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "def custom_batch_norm1d(input_tensor, weight, bias, eps):\n",
    "    # YOUR CODE HERE\n",
    "    normed_tensor = input_tensor\n",
    "    m = torch.mean(input_tensor, dim=0)\n",
    "    s = torch.var(input_tensor, dim=0, unbiased=False)\n",
    "    normed_tensor = ((input_tensor - m) / torch.sqrt(s + eps)) * weight + bias \n",
    "    return normed_tensor\n",
    "\n",
    "\n",
    "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
    "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm_out = batch_norm(input_tensor)\n",
    "custom_batch_norm_out = custom_batch_norm1d(input_tensor, batch_norm.weight.data, batch_norm.bias.data, eps)\n",
    "print(torch.allclose(batch_norm_out, custom_batch_norm_out) \\\n",
    "       and batch_norm_out.shape == custom_batch_norm_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-norm III\n",
    "\n",
    "Let's get rid of one more simplification - we implement the work of the batch-normalization layer at the prediction stage.\n",
    "\n",
    "At this stage, instead of batch statistics, we will use exponentially smoothed statistics from the layer's training history.\n",
    "\n",
    "In this step, you need to implement a full-fledged batch normalization class without using the standard function, which takes a two-dimensional tensor as input. Be careful, the calculation of the variance is based on a biased sample, and the calculation of the moving average is based on an unbiased one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[-0.1108,  2.5511,  0.7395],\n",
      "        [ 0.0340,  0.5591,  0.5695],\n",
      "        [ 0.2618,  0.7414,  0.2233],\n",
      "        [ 0.4852, -0.3341,  0.4275],\n",
      "        [ 0.6944, -1.2451,  1.0495]], grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[-0.1108,  2.5511,  0.7395],\n",
      "        [ 0.0340,  0.5591,  0.5695],\n",
      "        [ 0.2618,  0.7414,  0.2233],\n",
      "        [ 0.4852, -0.3341,  0.4275],\n",
      "        [ 0.6944, -1.2451,  1.0495]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "input_size = 3\n",
    "batch_size = 5\n",
    "eps = 1e-1\n",
    "\n",
    "\n",
    "class CustomBatchNorm1d:\n",
    "    def __init__(self, weight, bias, eps, momentum):\n",
    "        # YOUR CODE HERE\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.normed_tensor = None\n",
    "        self.training = True\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "        self.run_mean = 0\n",
    "        self.run_var = 1\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        # normalize tensor\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        if self.training:\n",
    "            self.mean = torch.mean(input_tensor, dim=0)\n",
    "            self.var = torch.var(input_tensor, dim=0, unbiased=False)\n",
    "            self.run_mean = (1 - self.momentum) * self.mean + self.momentum * self.run_mean\n",
    "            self.run_var = (1 - self.momentum) * torch.var(input_tensor, dim=0, unbiased=True) + self.momentum * self.run_var \n",
    "            self.normed_tensor = ((input_tensor - self.mean) / torch.sqrt(self.var + eps)) * self.weight + self.bias \n",
    "            return self.normed_tensor\n",
    "        else:\n",
    "            self.normed_tensor = ((input_tensor - self.run_mean) / torch.sqrt(self.run_var + eps)) * self.weight + self.bias\n",
    "            return self.normed_tensor\n",
    "\n",
    "\n",
    "    def eval(self):\n",
    "        # turn to eval()\n",
    "        # YOUR CODE HERE\n",
    "        self.training = False\n",
    "        return self.normed_tensor\n",
    "\n",
    "\n",
    "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
    "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.momentum = 0.5\n",
    "\n",
    "custom_batch_norm1d = CustomBatchNorm1d(batch_norm.weight.data,\n",
    "                                        batch_norm.bias.data, eps, batch_norm.momentum)\n",
    "\n",
    "all_correct = True\n",
    "\n",
    "for i in range(8):\n",
    "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "    norm_output = batch_norm(torch_input)\n",
    "    custom_output = custom_batch_norm1d(torch_input)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, atol=1e-04) \\\n",
    "        and norm_output.shape == custom_output.shape\n",
    "print(all_correct)\n",
    "\n",
    "batch_norm.eval()\n",
    "custom_batch_norm1d.eval()\n",
    "\n",
    "for i in range(8):\n",
    "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "    norm_output = batch_norm(torch_input)\n",
    "    custom_output = custom_batch_norm1d(torch_input)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, atol=1e-04) \\\n",
    "        and norm_output.shape == custom_output.shape\n",
    "print(norm_output)\n",
    "print(custom_output)\n",
    "print(all_correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaed9f255214391217c6b3ad51a795ae4188168fe90e39e87c85b9767c80efde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
